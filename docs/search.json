[
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "Objectives",
    "section": "",
    "text": "My goal with this write up is to provide examples of ways auxiliary data can be used to inform the speciation of sonar fish imaging data. This is part of an ongoing effort by the California (Coastal) Monitoring Plan (CMP) Sonar Subgroup to provide support to survey efforts using sonar to estimate abundances of migrating adult salmonids. Often distinguishing species for a given fish using a sonar image can be difficult due to images that are low-resolution and at an angle that makes identification difficult. The difficulty of accurate speciation can be compounded in systems when many different species are present at the same time, especially if those fish are of similar size classes and produce similar acoustic profiles.\nI do not aim to provide a “blanket” method for species apportionment that can be applied to all sonar imaging studies. Each watershed is different, and has different auxiliary data available for use. Also, the methods detailed do not aim to replace visual identifications made by scientists with local knowledge and identification experience. Instead, this document will provide examples of statistical methods that can utilize auxiliary data to aid in assigning species identifications to sonar movement data where visual identification is not possible. Some of these methods have been briefly described in Atkinson, Lacy, and Bellmer (2016), and I hope the following methodology provides additional helpful detail on how to implement the techniques.\nWe will begin with the most simple methods, incorporating the barest of auxiliary data sets, and build upon those with increasingly more complex methods and data sets. Hopefully by the end, the reader can better evaluate how to tackle their sonar imaging data and what auxiliary data they can use to increase the accuracy of species apportionment.\nThis will be a “living” document, with continues updates and edits being made while we continue to explore and test various speciation methods. Throughout this document, various chunks of R code will be presented in the following format:\n\nprint(\"hello reader\")\n\n[1] \"hello reader\"\n\n\nThese chunks are designed for users to copy and paste, or rewrite entirely, into their own R scripts to replicate the methods.\n\n\n\n\n\nReferences\n\nAtkinson, Kristine, Michael K Lacy, and Russell Bellmer. 2016. “Dual frequency identification sonar (DIDSON) deployment and preliminary performance as part of the California coastal salmonid monitoring plan.”"
  },
  {
    "objectID": "multinomial_regression.html",
    "href": "multinomial_regression.html",
    "title": "Multinomial Regression",
    "section": "",
    "text": "We can expand our speciation model to incorporate more than two species using a “multinomial logistic regression”. We can estimate the probability that a given sonar fish image \\(y\\) is species \\(k\\) using the equation:\n\\[\nP(y=k)=\\frac{e^{\\beta_{0,k}+\\beta_{1,k}*x_{1,y}+\\beta_{2,k}*x_{2,y}+...+\\beta_{M,k}*x_{M,y}}}{1+\\sum^{K-1}_{j=1}{e^{\\beta_{0,k}+\\beta_{1,k}*x_{1,y}+\\beta_{2,k}*x_{2,y}+...+\\beta_{M,k}*x_{M,y}}}}\n\\tag{1}\\]\nWhere \\(K\\) is the total number of different species \\(k\\) and \\(M\\) is the total number of explanatory variables. Similar to our binomial regression, the above relies on regression coefficients associated with each explanatory variable and the \\(k\\)th outcome. Again with our example will utilize explanatory variables of date of observation, fish length (cm), water flow (cfs). We’ll generate a new set of auxiliary data, this time for three species. This simulated auxiliary data is for three species “A”, “B”, and “C”, which have some amount of overlap in their run timing and sizes.\n\n#create new simulated auxiliary length and date for species C.\n\n#lengths\nlengths_C &lt;- rnorm(150, mean=110, sd=10)\n\n#dates\ndates_C &lt;- round(rnorm(150,mean = as.numeric(as.Date(\"2024-03-15\")), sd = 20))\n\n#make dataframe\naux_dataC &lt;- data.frame(\n  length = c(lengths_C),\n  date = round(as.Date(dates_C,origin = \"1970-01-01\")),\n  species = factor(rep(\"C\", times=150))\n)\n\n#join in flow we created in last section\naux_dataC&lt;-aux_dataC%&gt;%\n  dplyr::left_join(flow_df,by=\"date\")\n\naux_data&lt;-aux_data%&gt;%\n  rbind(aux_dataC)\n\n\n\n\n\n\nPlots showing date, length, and flow for expanded simulated auxiliary data with multiple species.\n\n\n\n\nRecall that the sonar_data we’ve been utilizing thus far is only for two species, so next we’ll add some data for a third species “C” into our set.\n\n#Simulate some example sonar data\n#lengths\nlengths_C &lt;- rnorm(300, mean=110, sd=10)\n\n#dates\ndates_C &lt;- round(rnorm(300,mean = as.numeric(as.Date(\"2024-03-15\")), sd = 20))\n\n#make dataframe\nsonar_dataC &lt;- data.frame(\n  length = c(lengths_C),\n  date = round(as.Date(dates_C,origin = \"1970-01-01\"))\n)\n\n#join in flow we created in last section\nsonar_dataC&lt;-sonar_dataC%&gt;%\n  dplyr::left_join(flow_df,by=\"date\")\n\nsonar_data&lt;-sonar_data%&gt;%\n  rbind(sonar_dataC)\n\nTo predict species we can use a method similar to our binomial logistic regression but this time using the multinom() call from the nnet package to run a multinomial regression model. Again will begin by iteratively training and testing the model on our auxiliary data set to estimate the models accuracy.\n\nsuppressMessages(library(nnet))\n# Split data into training and testing sets\nset.seed(Sys.time()) #reset seed\niterations=100\nresults&lt;-data.frame()\nfor(i in 1:iterations){\n  train_index &lt;- createDataPartition(aux_data$species, p = 0.7, list = FALSE) \n  train_data &lt;- aux_data[train_index, ]\n  test_data &lt;- aux_data[-train_index, ]\n  \n  #use multinom() call for multinomial regression\n  model_iter &lt;- multinom(species ~ as.numeric(date) + length + Flow_cfs,\n                    data = train_data)\n  test_data$species_predicted &lt;- predict(model_iter, newdata = test_data)\n  accuracy &lt;- sum(test_data$species_predicted == \n                    test_data$species)/nrow(test_data)\n  \n  Ntrue_A&lt;-sum(test_data$species==\"A\")\n  Ntrue_B&lt;-sum(test_data$species==\"B\")\n  Ntrue_C&lt;-sum(test_data$species==\"C\")\n  \n  Nest_A&lt;-sum(test_data$species_predicted==\"A\")\n  Nest_B&lt;-sum(test_data$species_predicted==\"B\")\n  Nest_C&lt;-sum(test_data$species_predicted==\"C\")\n\n  error_A &lt;- abs(Nest_A - Ntrue_A)\n  error_B &lt;- abs(Nest_B - Ntrue_B)\n  error_C &lt;- abs(Nest_C - Ntrue_C)\n\n  rel_error_A &lt;- error_A / Ntrue_A\n  rel_error_B &lt;- error_B / Ntrue_B\n  rel_error_C &lt;- error_C / Ntrue_C\n  \n  MAPE &lt;- mean(c(rel_error_A, rel_error_B, rel_error_C)) * 100\n  \n  d&lt;-data.frame(\"accuracy\"=accuracy,\"MAPE\"=MAPE)\n  results&lt;-results%&gt;%rbind(d)\n}\n\nBased on the above model training and iterative testing, we see our model predicted the species of our test data with an average accuracy of NA .\nNext we’ll retrain our model using the entire auxiliary data set, and then use it to predict species of sonar_data2.\n\nmodel_2&lt;-multinom(species ~ as.numeric(date) + length + Flow_cfs,\n                  data = aux_data)\n\n# weights:  15 (8 variable)\ninitial  value 961.285753 \niter  10 value 351.219577\niter  20 value 298.056623\niter  30 value 294.213467\niter  40 value 221.623902\niter  50 value 215.123009\niter  60 value 195.043939\niter  70 value 193.355963\niter  80 value 192.095513\niter  90 value 187.450502\niter 100 value 184.900676\nfinal  value 184.900676 \nstopped after 100 iterations\n\nsonar_data$species &lt;- predict(model_2, newdata = sonar_data, type = \"class\")\n\nWe can visualize our speciation results below, and see how they track with date and length.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPlot showing species assignments of sonar data using multinomial logistic regression.\n\n\n\n\n\nN_est4&lt;-sonar_data%&gt;%\n  group_by(species)%&gt;%\n  tally()\n\nOur final estimates of abundance in this example are 1039for species A, 785 for species B, and 326 for species C.\n\n\nNothing fancy here, we can replicate our bootstrapping methods we used during for the logistic regression model just this time with our multinomial model.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  d &lt;- sonar_data[sample(nrow(sonar_data), replace = TRUE), ]\n  p_classes &lt;- predict(model_2, newdata = d, type = \"class\")\n\n  sonar_boot&lt;-d%&gt;%cbind(p_classes)\n  \n  N_A&lt;-length(which(sonar_boot$p_classes==\"A\"))\n  N_B&lt;-length(which(sonar_boot$p_classes==\"B\"))\n  N_C&lt;-length(which(sonar_boot$p_classes==\"C\"))\n  iter&lt;-data.frame('iteration'=j,\"A\"=N_A[1],\"B\"=N_B[1],\"C\"=N_C[1])\n  results&lt;-results%&gt;%rbind(iter)\n}\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B),\n                   C=sum(C))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\nC_stats &lt;- quantile(iter_totals$C, probs = c(0.025, 0.975))\n\nThe above results show that our estimate of total abundance of species A in our sonar counts is 1039 with 95% CI [989, 1084], a count of 785 with 95% CI [730, 828] for species B, and a count of 326 with 95% CI [294, 353] for species C."
  },
  {
    "objectID": "multinomial_regression.html#estimating-uncertainty",
    "href": "multinomial_regression.html#estimating-uncertainty",
    "title": "Multinomial Regression",
    "section": "",
    "text": "Nothing fancy here, we can replicate our bootstrapping methods we used during for the logistic regression model just this time with our multinomial model.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  d &lt;- sonar_data[sample(nrow(sonar_data), replace = TRUE), ]\n  p_classes &lt;- predict(model_2, newdata = d, type = \"class\")\n\n  sonar_boot&lt;-d%&gt;%cbind(p_classes)\n  \n  N_A&lt;-length(which(sonar_boot$p_classes==\"A\"))\n  N_B&lt;-length(which(sonar_boot$p_classes==\"B\"))\n  N_C&lt;-length(which(sonar_boot$p_classes==\"C\"))\n  iter&lt;-data.frame('iteration'=j,\"A\"=N_A[1],\"B\"=N_B[1],\"C\"=N_C[1])\n  results&lt;-results%&gt;%rbind(iter)\n}\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B),\n                   C=sum(C))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\nC_stats &lt;- quantile(iter_totals$C, probs = c(0.025, 0.975))\n\nThe above results show that our estimate of total abundance of species A in our sonar counts is 1039 with 95% CI [989, 1084], a count of 785 with 95% CI [730, 828] for species B, and a count of 326 with 95% CI [294, 353] for species C."
  },
  {
    "objectID": "inseason_cutoff.html",
    "href": "inseason_cutoff.html",
    "title": "In-season Cutoff",
    "section": "",
    "text": "Perhaps the simplest method to speciate between two different fish species in a study would be to incorporate historical run-time and catch/escapement data proximate to the sonar study area in a method we can refer to as an “in-season cutoff”(Nolan et al. 2023). Here we’ll simulate some historical auxiliary count data for two different species, “A” and “B”, using the same methods as above to get the following weekly_aux data.\n\nhead(weekly_aux)\n\n# A tibble: 6 × 3\n  Week       species Total_Count\n  &lt;date&gt;     &lt;chr&gt;         &lt;int&gt;\n1 2009-09-21 A                 2\n2 2009-09-28 A                 6\n3 2009-10-05 A                14\n4 2009-10-12 A                25\n5 2009-10-19 A                31\n6 2009-10-26 A                58\n\n\nThis auxiliary data set shows the weekly counts of our two target species for a run 13 years prior to our sonar data at a proximate location. Obviously a more proximate (in both space and time) auxiliary data set could be more representative of our sonar samples, and thus more applicable. However, often the auxiliary data available is not the most optimal, and so we have to use the best data available.\nThis historical data set can be used to produce an “in-season cutoff”, essentially finding a date in the historical data when the proportion of species B is greater than species A. We can then apply that date to our sonar data, and assign any fish observed before that date as “A” and after as “B”. To utilize an “in-season cutoff” we need to find the week when the proportion of species B captured was higher than the count of species A. We can use the following code to first estimate the proportion of each species for each week fish were caught:\n\n# Calculate proportions and reshape in one step\nproportions_combined &lt;- weekly_aux %&gt;%\n  group_by(Week) %&gt;%\n  mutate(total_n = sum(Total_Count)) %&gt;%\n  mutate(proportion = Total_Count / total_n) %&gt;%\n  select(Week, species, proportion) %&gt;%\n  pivot_wider(names_from = species,\n              values_from = proportion,\n              names_prefix = \"proportion_\") %&gt;%\n  ungroup()\n\nNow we can easily find the first week in our data when the count of species B is greater than species A.\n\n# Find the first week where the proportion of species A is greater than B\nresult &lt;- proportions_combined %&gt;%\n  filter(proportion_B &gt; proportion_A) %&gt;%\n  slice(1)\n\nresult_week &lt;- result$Week\n\n# Output the first week\nresult_week\n\n[1] \"2009-12-07\"\n\n\nThe above shows us that 2009-12-07 was the first week in which the count of species B was greater than the count of A in our auxiliary data.\nWe can plot the distribution of the data here along with our cutoff:\n\n\n\n\n\nPlot showing species counts of auxiliary historical hatchery data set. Dashed line represents ‘in-season cutoff’.\n\n\n\n\nUsing this historical auxiliary data set, we can decide to set our cutoff to the date of 2009-12-07, and assign any fish detection in our sonar data before that date as species A, and any after as species B.\n\ntarget_year&lt;-2023\ncutoff_date&lt;-as.Date(paste(target_year,\n                           month(result_week),day(result_week),sep=\"-\"))\n\ndaily_sonar&lt;-daily_sonar\ndaily_sonar$predicted_species&lt;-\n  ifelse(daily_sonar$date&lt;cutoff_date,'A','B')\n\nWe can now plot our sonar data and show the species assignments.\n\n\n\n\n\nPlot showing species assignments of sonar data using the ‘In-season cutoff’ method\n\n\n\n\nFinally we can estimate counts for each species in our sonar data.\n\nN_est&lt;-daily_sonar%&gt;%\n  group_by(predicted_species)%&gt;%\n  summarise(total=sum(Net_Movement))\n\nWith the above snippet we can see that our final estimates of sonar counts are 922 for species A and 1078 for species B. We know that our simulated data had a count of 116 for species A and 103 for species B, so we had an error rate of -694.8275862%.\nThe above method of speciation, while simple, provides a straightforward approach for leveraging historical data to infer species composition in sonar detection studies. Key assumptions to realize when utilizing this method are:\n\nAll fish being speciated are either one of two species.\nThere is no overlap in run timing.\nDate based threshold of the auxiliary data is representative of the sonar site.\nDate based threshold used is consistent between years.\n\nReal world systems will likely violate one or more of the above assumptions, so the use of this method is likely to bias species identifications and resulting count estimates. Our simulation had an overlap in run timing and variation in the distributions that resulted in an over-count of species B and an under-count of species A.\n\n\n\n\nReferences\n\nNolan, Jesse, Erika Partee, Jennifer Jacobs, and Ryan Nelson. 2023. “2021-2022 Sonar-derived estimates of Chinook Salmon Prepared by :” April. Tolowa Dee-ni’ Nation."
  },
  {
    "objectID": "further.html",
    "href": "further.html",
    "title": "Further Info",
    "section": "",
    "text": "What apportionment method is right for you?\nThis methods documentation does not aim to dictate to sonar surveys the “correct” method to assign species to sonar fish counts. Choosing the right species apportionment method depends on the available auxiliary data, the degree of overlap in species covariate distributions (length, date, etc.), and the desired level of certainty. Simple methods like in-season cutoffs may be appropriate for systems with well-separated run timings, while in-season species proportioning provides a better alternative when some overlap exists. Logistic regression models allow for the incorporation of additional covariates, such as fish length and environmental variables, improving accuracy. For studies involving multiple species with overlapping distributions, multinomial logistic regression or Gaussian mixed models can offer robust solutions for species ID.\n\nThere is no one-model-fits-all solution for speciating sonar counts.\nThe more complex your fish community, the more complex the methods, and the more detailed the data needed.\nThe methods available to you depends on the detail and proximity of the auxiliary data available for inference.\n\n\n\nPotential future methods\nThe speciation methods that have been reviewed are far from an exhaustive list of techniques that can utilize auxiliary data to inform sonar count species assignments. As the CMP Sonar Subgroup continues to explore methods of utilizing sonar imaging to enumerate migrating salmon, we hope to expand upon this list. Some potential avenues for further exploration include:\n\nGeneralized Additive Models (GAMs) to capture non-linear relationships between covariates and species identity.\nIncorporating acoustic telemetry as an auxiliary data set to expand escapement estimates (Maxwell, Buck, and Faulkner 2019).\nUtilizing a Bayesian framework to incorporate informative prior knowledge and Bayesian updating as more data becomes available Fleischman and Burwen (2003).\nExploring machine learning methods used to process sonar imagery files to also incorporate statistical methods for automatic species assignment.\n\n\n\n\n\n\nReferences\n\nFleischman, Steve J., and Debby L. Burwen. 2003. “Mixture models for the species apportionment of hydroacoustic data, with echo-envelope length as the discriminatory variable.” ICES Journal of Marine Science 60 (3): 592–98. https://doi.org/10.1016/S1054-3139(03)00041-9.\n\n\nGrote, Ann B., Michael M. Bailey, Joseph D. Zydlewski, and Joseph E. Hightower. 2014. “Multibeam sonar (DIDSON) assessment of American shad (Alosa sapidissima) approaching a hydroelectric dam.” Canadian Journal of Fisheries and Aquatic Sciences 71 (4): 545–58. https://doi.org/10.1139/cjfas-2013-0308.\n\n\nMaxwell, Suzanne, Greg Buck, and April Faulkner. 2019. “Using Acoustic Telemetry to Expand Sonar Escapement Indices of Chinook Salmon to In-river abundance estimates.” Alaska Department of Fish; Game. https://www.sciencedirect.com/science/article/abs/pii/S0165783619302024."
  },
  {
    "objectID": "binomial_regression.html",
    "href": "binomial_regression.html",
    "title": "Binomial Regression",
    "section": "",
    "text": "The more auxiliary data we have available to us, the more complex, and hopefully accurate, of a model we can use to assign species to our sonar data. Let’s add some variables to our auxiliary data set, notably fish length (cm) and daily flow data. For our example we’ll just use one year of simulated auxiliary data, but you could use multiple years of auxiliary data to model speciation. We’ll use similar methods as before, and can visualize the simulated data that we’ll use here:\n#create new simulated auxiliary length, date, and flow data.\nset.seed(42)  # For reproducibility\n#lengths\nlengths_A &lt;- round(rnorm(400, mean=100, sd=15),2)\nlengths_B &lt;- round(rnorm(325, mean=80, sd=15),2)\n\n#dates\ndates_A &lt;- round(rnorm(400,mean = as.numeric(as.Date(\"2023-11-20\")), sd = 20))\ndates_B &lt;- round(rnorm(325,mean = as.numeric(as.Date(\"2024-01-10\")), sd = 22))\n\n#make dataframe\naux_data &lt;- data.frame(\n  length = c(lengths_A, lengths_B),\n  date = round(as.Date(c(dates_A,dates_B), origin = \"1970-01-01\")),\n  species = factor(rep(c(\"A\", \"B\"), times=c(400,325)))\n)\n\n#flow\nstart_date &lt;- as.Date(paste(year(min(aux_data$date)),\"01\",\"01\",sep=\"-\"))\nend_date &lt;- as.Date(paste(year(max(aux_data$date)),\"12\",\"31\",sep=\"-\"))\ndates &lt;- seq.Date(start_date, end_date, by = \"day\")\n\ndays_in_period &lt;- length(dates)\nmax_flow &lt;- 1000  # maximum flow in cfs\nmin_flow &lt;- 100    # minimum flow in cfs\n\n# Create a sinusoidal flow pattern to simulate seasonal variation\nflow_pattern &lt;- (max_flow - min_flow) / 2 * \n  sin(2 * pi * (1:days_in_period -20) / 365) + \n  (max_flow + min_flow) / 2\n\n# Create a random noise process using an auto-regression model\n# rho=level of autocorrelation\nrho &lt;- 0.9  # autocorrelation parameter; \n#higher values give smoother transitions\n\nac_noise &lt;- numeric(days_in_period)\nac_noise[1] &lt;- rnorm(1, mean = 0, sd = 100)  # initial noise value\nfor (i in 4:days_in_period) {\n  ac_noise[i] &lt;- rho * ac_noise[i - 1] + rnorm(1, mean = 0, sd = 100)\n}\nflow_data &lt;- flow_pattern + ac_noise\n\n# Ensure no flow goes below the minimum flow\nflow_data[flow_data &lt; 10] &lt;- 10\n\n# Create a dataframe for plotting and analysis\nflow_df &lt;- data.frame(\n  date = as.Date(dates),\n  Flow_cfs = flow_data\n)\n\naux_data&lt;-aux_data%&gt;%\n  dplyr::left_join(flow_df,by=\"date\")\nPlots showing date, length, and flow for expanded simulated auxiliary data.\nWe can use this auxiliary data to build a binomial logistic regression model, in which our response variable is the probability of the observed fish being one of two species. This logistic regression method can allow us to incorporate additional covariates beyond just date of capture. For our auxiliary data we can represent this model with the following equation:\n\\[\nP(y=A)=\\frac{1}{1+e^{\\beta_0+\\beta_1*x_{1,y}+\\beta_2*x_{2,y}+....+\\beta_M*x_{M,y}}}\n\\tag{1}\\]\nWhere \\(P(y=A)\\) is the probability of a given fish \\(y\\) being species \\(A\\), \\(\\beta\\) is the regression coefficient for a given explanatory variable, and \\(M\\) is the total number of explanatory variables. Here we’ll be using three potential explanatory variables: date of observation, fish length (cm), and average daily water flow (cfs). This is similar to methods outlined in Metheny (2012), where models were developed using live fish observations from the USGS Cooperative Fish and Wildlife Research Unit on Redwood Creek from 2009-2010.\nWe can build this model and validate it before we attempt to assign species to any sonar data. We can start by assigning a species index to each record, of 1 if the species was A, 0 if B. Then we can split our data into a “training” data set to build the model on, and another data set to test the model on. We’ll use the createDataPartition() function from the caret package to split our auxiliary data into the train_data data frame comprised of 70% of our records, and a test_data data frame comprising the other 30%.\naux_index&lt;-aux_data%&gt;%\n  mutate(species_index=ifelse(aux_data$species==\"A\",1,0))\n\n# Split data into training and testing sets\nset.seed(123) #set seed for repeatability\n#create training data set with 70% of data\ntrain_index &lt;- createDataPartition(aux_index$species,\n                                   p = 0.7, list = FALSE) \ntrain_data &lt;- aux_index[train_index, ]\ntest_data &lt;- aux_index[-train_index, ]\nNow that we have our training and testing data sets, we can build our model with the glm() function, structuring it off of Equation 1, and setting as a binomial regression by setting family=binomial.\nmodel_1 &lt;- glm(species_index ~ as.numeric(date) + length + Flow_cfs,\n               data = train_data, family = binomial)\nsummary(model_1)\n\n\nCall:\nglm(formula = species_index ~ as.numeric(date) + length + Flow_cfs, \n    family = binomial, data = train_data)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.45746  -0.13241   0.03463   0.23579   2.96957  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       2.226e+03  2.656e+02   8.383  &lt; 2e-16 ***\nas.numeric(date) -1.134e-01  1.350e-02  -8.399  &lt; 2e-16 ***\nlength            8.964e-02  1.423e-02   6.299 2.99e-10 ***\nFlow_cfs         -1.264e-03  9.319e-04  -1.356    0.175    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 698.91  on 507  degrees of freedom\nResidual deviance: 194.90  on 504  degrees of freedom\nAIC: 202.9\n\nNumber of Fisher Scoring iterations: 7\nIf we look at the summary output of our model, we can see that both length and date have significant effects on the probability of a fish being species A or species B. The flow covariate was not significant in species ID, so we can actually drop it from our model moving forward.\nmodel_1 &lt;- glm(species_index ~ as.numeric(date) + length,\n               data = train_data, family = binomial)\nWe then use the predict() function to predict the species assignments of the test_data using the model we created. This assigns a probability of each test record being species A, which we can then round and assign a value of 1 if that the probability is greater then 50%, and 0 otherwise. This process is a “threshold” assignment, in which we categorize our predicted probabilities into binary classes. Then those predictions are rejoined to the test_data.\n# Predictions\npredictions &lt;- predict(model_1, newdata = test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nspecies_predicted&lt;-ifelse(predicted_classes==1,\"A\",\"B\")\n\ntest_data&lt;-test_data%&gt;%cbind(species_predicted)\nWe can now compare the predicted species for the test_data to the actual species, and see how accurate our species identification was using this model.\n# Accuracy\naccuracy &lt;- sum(species_predicted == test_data$species) / nrow(test_data)\nprint(paste(\"Accuracy:\", round(accuracy, 3)))\n\n[1] \"Accuracy: 0.917\"\nAbove we can see our estimate accuracy in determining if a given fish was species A or B was 0.917. Of course, for our purpose we are not necessarily interested in whether or not a given sonar fish image is one species or another. What we are most interested in is abundances of each species based on our sonar data.\nTo figure out how much error there is in our estimates of the abundance for each species we can find the true count for each species in our test_data and compare it to the abundance estimate based on the species predictions from the model.\nNtrue_A&lt;-sum(test_data$species==\"A\")\nNtrue_B&lt;-sum(test_data$species==\"B\")\n\nNest_A&lt;-sum(test_data$species_predicted==\"A\")\nNest_B&lt;-sum(test_data$species_predicted==\"B\")\n\nerror_A &lt;- abs(Nest_A - Ntrue_A)\nerror_B &lt;- abs(Nest_B - Ntrue_B)\n\nrel_error_A &lt;- error_A / Ntrue_A\nrel_error_B &lt;- error_B / Ntrue_B\n  \nMAPE &lt;- mean(c(rel_error_A, rel_error_B)) * 100\nWe see above that testing our model shows an error of 3.729 % in the our species abundance predictions.\nNext we’ll want to do the above many more times in an iterative process similar to the bootstrapping we’ve already done, and we can use our average relative error in abundance estimates as our benchmark.\nset.seed(Sys.time()) #reset seed\niterations=100\nresults&lt;-data.frame()\nfor(i in 1:iterations){\n  train_index &lt;- createDataPartition(aux_index$species,\n                                     p = 0.7, list = FALSE) \n  train_data &lt;- aux_index[train_index, ]\n  test_data &lt;- aux_index[-train_index, ]\n  model_iter &lt;- glm(species_index ~ as.numeric(date) + length,\n               data = train_data, family = binomial)\n  predictions &lt;- predict(model_iter, \n                         newdata = test_data, type = \"response\")\n  predicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n  species_predicted&lt;-ifelse(predicted_classes==1,\"A\",\"B\")\n\n  test_data&lt;-test_data%&gt;%\n    cbind(species_predicted&lt;-ifelse(predicted_classes==1,\"A\",\"B\"))\n  accuracy &lt;- sum(species_predicted == test_data$species)/nrow(test_data)\n  \n  Ntrue_A&lt;-sum(test_data$species==\"A\")\n  Ntrue_B&lt;-sum(test_data$species==\"B\")\n\n  Nest_A&lt;-sum(test_data$species_predicted==\"A\")\n  Nest_B&lt;-sum(test_data$species_predicted==\"B\")\n\n  error_A &lt;- abs(Nest_A - Ntrue_A)\n  error_B &lt;- abs(Nest_B - Ntrue_B)\n\n  rel_error_A &lt;- error_A / Ntrue_A\n  rel_error_B &lt;- error_B / Ntrue_B\n  \n  MAPE &lt;- mean(c(rel_error_A, rel_error_B)) * 100\n  \n  d&lt;-data.frame(\"accuracy\"=accuracy,\"MAPE\"=MAPE)\n  results&lt;-results%&gt;%rbind(d)\n}\nBased on the above model training and iterative testing, we see our model predicted the species of our test data with an average accuracy of 0.911 and an average error in abundance estimates of 3.113 %.\nWe’ve trained and tested our model, and have some benchmarks of accuracy in species ID and abundance estimates. Next we’ll have to generate a new sonar data set for this example that will incorporate length and flow data linked to our sonar counts. We can simulate this data similar to how we’ve done for our prior two examples here:\n#Simulate some example sonar data\nset.seed(123)  # For reproducibility\n#lengths\nlengths_A &lt;- round(rnorm(1000, mean=100, sd=15),2)\nlengths_B &lt;- round(rnorm(850, mean=80, sd=15),2)\n\n#dates\ndates_A &lt;- round(rnorm(1000,mean=as.numeric(as.Date(\"2023-11-10\")),sd=22))\ndates_B &lt;- round(rnorm(850,mean=as.numeric(as.Date(\"2024-01-12\")),sd=20))\n\n#make dataframe\nsonar_data &lt;- data.frame(\n  length = c(lengths_A, lengths_B),\n  date = round(as.Date(c(dates_A,dates_B), origin = \"1970-01-01\"))\n)\n\n#join in flow data\nsonar_data&lt;-sonar_data%&gt;%\n  dplyr::left_join(flow_df,by=\"date\")\nNext we’ll retrain our model using the entire auxiliary data set.\nmodel_1&lt;-glm(species_index ~ as.numeric(date) + length + Flow_cfs,\n             data = aux_index, family = binomial)\nBased on the summary output for our model, we see that length and date are both strong predictors of species, while the flow covariate is not significant in predicting species.\nNext we can use our model_1 to assign species to our sonar data.\npredictions &lt;- predict(model_1, newdata = sonar_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\nsonar_predicted&lt;-sonar_data%&gt;%cbind(predicted_classes)\nsonar_predicted$species=ifelse(sonar_predicted$predicted_classes==1,\"A\",\"B\")\nPlot the speciation results.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPlot showing species assignments of sonar data using binomial logistic regression.\nN_est3&lt;-sonar_predicted%&gt;%\n  group_by(species)%&gt;%\n  tally()\nOur final estimates of abundance in this example are 1038 for species A and 812 for species B."
  },
  {
    "objectID": "binomial_regression.html#estimating-uncertainty",
    "href": "binomial_regression.html#estimating-uncertainty",
    "title": "Binomial Regression",
    "section": "Estimating uncertainty",
    "text": "Estimating uncertainty\nWe can utilize a bootstrapping method again to incorporate variance in our sonar data and estimate confidence intervals for our final species abundance estimates. We’ll do this by iteratively rebuilding our logistic model by resampling our sonar data. The following chunk uses a for-loop to sample, with replacement, our sonar_data, and then assign species using model_1 to the new data set. We then calculate new estimates of abundance for each species with the same methods we just used.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  d &lt;- sonar_data[sample(nrow(sonar_data), replace = TRUE), ]\n  p_boot &lt;- predict(model_1, newdata = d, type = \"response\")\n  p_classes &lt;- ifelse(p_boot &gt; 0.5, 1, 0)\n\n  sonar_boot&lt;-sonar_data%&gt;%cbind(p_classes)\n  sonar_boot$species=ifelse(sonar_boot$p_classes==1,\"A\",\"B\")\n  N_A&lt;-length(which(sonar_boot$species==\"A\"))\n  N_B&lt;-length(which(sonar_boot$species==\"B\"))\n  iter&lt;-data.frame('iteration'=j,\"A\"=N_A[1],\"B\"=N_B[1])\n  results&lt;-results%&gt;%rbind(iter)\n}\n\nWe can use the results output from above to calculate our 95% confidence intervals using the quantile() call:\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\n\nThe above results show that our estimate of total abundance of species A in our sonar counts is 1038 with 95% CI [993, 1069] and a count of 812 with 95% CI [782, 858] for species B.\nLogistic regression is a fairly simple method that can incorporate multiple covariates to help in assigning species to sonar counts. A good advantage of this method is we can train and test our logistic model with our auxiliary data to estimate accuracy, and again bootstrap the sonar data to assign confidence intervals. Key assumption for the binomial logistic regression include:\n\nAll fish being speciated either one of two species.\nThe dates of run times and lengths of fish observed in the auxiliary data are representative of the fish observed in sonar imaging."
  },
  {
    "objectID": "data_review.html",
    "href": "data_review.html",
    "title": "Simulating Sonar Data",
    "section": "",
    "text": "Before we review speciation methods, we can set the baseline of what our sonar data set will look like. For a given period of survey, we can expect to have a data set that tells us the date, and the net movement of detections. For the examples in this document I’ll be simulating both sonar and auxiliary data sets for ease of use and replication. I’ll give a general outline in this section on how the data is simulated.\nThere a few R packages used in this documentation that can be loaded here:\n\n#load packages\nrm( list = ls()) #clear env\nlibrary(tidyverse) #for piping and data formatting ease\nlibrary(ggplot2) #for plotting\nlibrary(lubridate) #makes formatting dates a little easier\nlibrary(caret) #provides the createDataPartition() call for testing/training\nlibrary(nnet) #for multi-nomial regression\nlibrary(mclust) #for gaussian mixed models\n\nWe’ll start by simulating a simpler sonar detection data set that involves two species with overlapping distributions of run timing. First we can simulate our fish movement data as having runs with normal distributions centered around two peak dates, which we’ll define as peakA and peakB. We’ll also make these dates and the date range numeric values so we can build distributions from them.\n\n# Identify peaks\npeakA &lt;- as.numeric(as.Date(\"2023-11-10\")) \npeakB &lt;- as.numeric(as.Date(\"2024-01-12\"))\n\nWe can then create normal distributions of movement data centered around our peaks, and combine the data into one data set combined_counts, which we can then turn back into a date format for easy plotting. For this example we’ll also assign each record a “true” species identification which we can use to test the accuracy of our methods.\n\n# Simulate counts around the peaks using normal distributions \nset.seed(42)  # For reproducibility \nnA=1000\nnB=850\ncountsA &lt;- data.frame(species=\"A\",\n                      date=round(rnorm(n=nA, mean = peakA, sd = 22)))\ncountsB &lt;- data.frame(species=\"B\",\n                      date=round(rnorm(n=nA, mean = peakB, sd = 20)))  \n# Combine the counts \ncombined_counts &lt;- countsA%&gt;%rbind(countsB)\n#combine distributions \ncombined_counts$date&lt;-as.Date(combined_counts$date,origin = \"1970-01-01\") \n\nNext we’ll convert the data into the daily_sonar data frame and do some additional formatting.\n\n# Create a dataframe with counts \ndaily_sonar &lt;- combined_counts%&gt;%\n  group_by(species,date)%&gt;%\n  tally()\nnames(daily_sonar) &lt;- c(\"species\",\"date\", \"Net_Movement\")  \n\nNow we can bin daily counts by week for easier plotting and visualization.\n\n# Bin the dates by week \ndaily_sonar$Week &lt;- as.Date(cut(daily_sonar$date, breaks = \"week\",  \n                                start.on.monday = TRUE))\n\nOur final daily_sonar data frame contains four columns: the “true” species ID we gave the count, the date of the observation, the Net_Movement (essentially count in this case), and the week. Remember that while we gave each simulated observation a “true” species code, either A or B, we wouldn’t actually know that information in our real sonar data sets. We will be using the simulated species ID to test our speciation methods accuracy in the following sections.\n\nhead(daily_sonar)\n\n# A tibble: 6 × 4\n# Groups:   species [1]\n  species date       Net_Movement Week      \n  &lt;chr&gt;   &lt;date&gt;            &lt;int&gt; &lt;date&gt;    \n1 A       2023-08-28            1 2023-08-28\n2 A       2023-09-05            2 2023-09-04\n3 A       2023-09-06            1 2023-09-04\n4 A       2023-09-12            1 2023-09-11\n5 A       2023-09-13            1 2023-09-11\n6 A       2023-09-15            1 2023-09-11\n\n\nLet’s plot the sonar data to see how it’s distributed over our simulated survey period. We see two run peaks centered around the dates we set, and what looks like quite a bit of overlap in the the simulated detection timing.\n\n\n\n\n\nPlot showing distribution of simulated sonar net movement detections."
  },
  {
    "objectID": "gmm.html",
    "href": "gmm.html",
    "title": "Gaussian Mixed Modelling",
    "section": "",
    "text": "An alternative to the logistic models is called a “Guassian Mixed Model” (GMM). The logistic regression models used previously involve assigning individuals to one species or another based on whether or not the probability exceeds a threshold. When their is a lot of overlap in distribution of lengths or dates, this species assignment method is subject to bias, especially when proportions of a given species are relatively one-sided (Fleischman and Burwen 2003). While a GMM approach may not be as easily interpretable as the logistic regressions, and rely on an assumption of normality, they can handle uncertainty better, especially when dealing with significant overlap in species length and timing distributions.\nA general equation of the GMM utilized is:\n\\[f(x,y)=\\sum_{k=1}^{K}\\pi_k*N((x,y)|\\mu_k,\\sigma_k^2) \\]\nThe above gives the probability density function of a dataset as a weighted sum of multiple Gaussian (normal) distributions. Each component \\(k\\) represents a cluster in the data,of which there are a total of \\(K\\), for our example we expect 3 clusters for species “A”, “B”, and “C”. \\((x,y)\\) are the observed data points (in our case length and date), and \\(\\pi_k\\) is the mixing proportion for component \\(k\\), which all sum to 1. \\(N((x,y)|\\mu_k,\\sigma_k^2)\\) represents the Gaussian distribution for each of the \\(k\\)-th components with a mean of \\(\\mu_k\\) and a variance of \\(\\sigma_k^2\\).\nFor our demonstration, we can use the same auxiliary and sonar data sets that we’ve already simulated before, and see how the GMM compares to the logistic regression methods we’ve used. We’ve already seen the distributions of these data, but we can visualize the 2D distribution as well:\n\n\n\n\n\nPlot of the 2D distributions of auxiliary data with species identities.\n\n\n\n\nWith the above plot we can see distinct clusters of dates and lengths, with some overlap between species.\nWe’ll start by loading the mclust package to utilize their Mclust() call to run a GMM on our data, and testing the model on the auxiliary data in the same way we’ve used in before. Note that in this for-loop, instead of using the predict function to set a given species ID based on a probability threshold, I’m estimating the abundance by summing the total probability for each species and using that as our abundance estimate.\n\nset.seed(Sys.time()) #reset seed\niterations=100\nresults&lt;-data.frame()\ngmm_data&lt;-select(aux_data,length,date,Flow_cfs,species)\n#note here that the Mclust requires date in a numeric format\ngmm_data$num_date&lt;-as.numeric(gmm_data$date) \nfor(i in 1:iterations){\n  train_index &lt;- createDataPartition(gmm_data$species, p = 0.7, list = FALSE) \n  train_data &lt;- gmm_data[train_index, ]\n  test_data &lt;- gmm_data[-train_index, ]\n  \n  Ntrue_A&lt;-sum(test_data$species==\"A\")\n  Ntrue_B&lt;-sum(test_data$species==\"B\")\n  Ntrue_C&lt;-sum(test_data$species==\"C\")\n  \n  #Fit a Gaussian Mixture Model to the training data \n  model_iter &lt;- Mclust(train_data[, c(\"length\", \"num_date\")], G=3)\n  \n  #summary(model_iter)\n  # Predict species for the test data based on the trained model\n  predictions &lt;- predict(model_iter, test_data[, c(\"length\", \"num_date\")])\n  test_data$species_predicted &lt;- factor(predictions$classification,\n                                        levels = 1:3, labels = c(\"A\",\"B\",\"C\"))\n  \n  accuracy &lt;- sum(test_data$species_predicted == \n                    test_data$species)/nrow(test_data)\n  \n  #Change in abundance estimate:\n  #here I can sum the probability for each classification to \n  #estimate total abundance\n  #instead of using the threshold classification method\n  Nest_A&lt;-sum(predictions$z[,1])\n  Nest_B&lt;-sum(predictions$z[,2])\n  Nest_C&lt;-sum(predictions$z[,3])\n\n  error_A &lt;- abs(Nest_A - Ntrue_A)\n  error_B &lt;- abs(Nest_B - Ntrue_B)\n  error_C &lt;- abs(Nest_C - Ntrue_C)\n\n  rel_error_A &lt;- error_A / Ntrue_A\n  rel_error_B &lt;- error_B / Ntrue_B\n  rel_error_C &lt;- error_C / Ntrue_C\n  \n  MAPE &lt;- mean(c(rel_error_A, rel_error_B, rel_error_C)) * 100\n  d&lt;-data.frame(\"accuracy\"=accuracy,\"MAPE\"=MAPE)\n  \n  results&lt;-results%&gt;%rbind(d)\n}\n\nBased on the above model training and iterative testing, we see our model predicted the species of our test data with an average accuracy of 0.886, and estimated species abundance with an average error of 9.205.\nNow we can retrain our model using the entire auxiliary data set, and use it to predict species counts for our sonar_data.\n\nmodel_3&lt;-Mclust(gmm_data[, c(\"length\", \"num_date\")], G=3)\n#note here that the Mclust requires date in a numeric format\nsonar_data$num_date&lt;-as.numeric(sonar_data$date)\npredictions &lt;- predict(model_3, sonar_data[, c(\"length\", \"num_date\")])\nsonar_data$species_predicted &lt;- factor(predictions$classification,\n                                       levels = 1:3, labels = c(\"A\",\"B\",\"C\"))\n\nWe can plot our predicted species to see if the clustering generally fits that of our auxiliary data.\n\n\n\n\n\nPlot of the 2D distributions of sonar data with predicted species identities.\n\n\n\n\nNow, instead of using the assigned species values, we’ll use the probability each fish is a given species, which will give us a total estimate of abundance. If we look at the predictions$z we see a record for each observation that gives the probability it is one of the three classes based on the model predictions.\n\nhead(predictions$z)\n\n             1            2            3\n[1,] 0.9999889 1.106059e-05 3.037456e-15\n[2,] 0.8699173 1.300606e-01 2.211427e-05\n[3,] 0.9484408 4.313523e-02 8.423995e-03\n[4,] 0.9992204 7.795544e-04 8.353405e-10\n[5,] 0.9856517 1.434752e-02 7.578460e-07\n[6,] 0.9998748 1.251561e-04 3.517768e-08\n\n\nWe can take the sum of each column to estimate the abundance of each species here:\n\nNest_A&lt;-round(sum(predictions$z[,1]))\nNest_B&lt;-round(sum(predictions$z[,2]))\nNest_C&lt;-round(sum(predictions$z[,3]))\n\nOur estimates of abundance in this example are 1082 for species A, 741 for species B, and 327 for species C.\n\n\nAgain we can replicate our bootstrapping methods we used to estimate uncertainty in prior methods.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  d &lt;- sonar_data[sample(nrow(sonar_data), replace = TRUE), ]\n  predictions &lt;- predict(model_3, d[, c(\"length\", \"num_date\")])\n  d$species_predicted &lt;- factor(predictions$classification,\n                                       levels = 1:3, labels = c(\"A\",\"B\",\"C\"))\n  \n  N_A&lt;-sum(predictions$z[,1])\n  N_B&lt;-sum(predictions$z[,2])\n  N_C&lt;-sum(predictions$z[,3])\n  iter&lt;-data.frame('iteration'=j,\"A\"=N_A[1],\"B\"=N_B[1],\"C\"=N_C[1])\n  results&lt;-results%&gt;%rbind(iter)\n}\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B),\n                   C=sum(C))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\nC_stats &lt;- quantile(iter_totals$C, probs = c(0.025, 0.975))\n\nThe above results show that our estimate of total abundance of species A in our sonar counts is 1082 with 95% CI [1042, 1125], a count of 741 with 95% CI [702, 785] for species B, and a count of 327 with 95% CI [295, 355] for species C."
  },
  {
    "objectID": "gmm.html#estimating-uncertainty",
    "href": "gmm.html#estimating-uncertainty",
    "title": "Gaussian Mixed Modelling",
    "section": "",
    "text": "Again we can replicate our bootstrapping methods we used to estimate uncertainty in prior methods.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  d &lt;- sonar_data[sample(nrow(sonar_data), replace = TRUE), ]\n  predictions &lt;- predict(model_3, d[, c(\"length\", \"num_date\")])\n  d$species_predicted &lt;- factor(predictions$classification,\n                                       levels = 1:3, labels = c(\"A\",\"B\",\"C\"))\n  \n  N_A&lt;-sum(predictions$z[,1])\n  N_B&lt;-sum(predictions$z[,2])\n  N_C&lt;-sum(predictions$z[,3])\n  iter&lt;-data.frame('iteration'=j,\"A\"=N_A[1],\"B\"=N_B[1],\"C\"=N_C[1])\n  results&lt;-results%&gt;%rbind(iter)\n}\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B),\n                   C=sum(C))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\nC_stats &lt;- quantile(iter_totals$C, probs = c(0.025, 0.975))\n\nThe above results show that our estimate of total abundance of species A in our sonar counts is 1082 with 95% CI [1042, 1125], a count of 741 with 95% CI [702, 785] for species B, and a count of 327 with 95% CI [295, 355] for species C."
  },
  {
    "objectID": "inseason_proportion.html",
    "href": "inseason_proportion.html",
    "title": "In-season species proportion",
    "section": "",
    "text": "Let’s use our simulated data daily_sonar data again, however this time our auxiliary counts will be from the same year as our sonar data. This method determines the proportion of counts for each species in a given period of sonar data based on the proportions seen in a proximate (spatially and temporally) auxiliary data set. This is similar to methods used in Nolan et al. (2023), Mora et al. (2018), and suggested in Boughton, Nelson, and Lacy (2022). The following simulated data is produced in the same manner as the historical auxiliary data we created, but this time representing count data collected the same season as our sonar data.\nset.seed(123)  # For reproducibility\n# Simulate counts around the peaks using normal distributions\nh_countsA &lt;- round(rnorm(400, mean = as.numeric(as.Date(\"2023-11-20\")),\n                         sd = 20))\nh_countsB &lt;- round(rnorm(325, mean = as.numeric(as.Date(\"2024-01-10\")),\n                         sd = 22))\n\n# Create dataframes, add species columns, and combine in one step\ndate_counts &lt;- bind_rows(\n  as.data.frame(table(as.Date(h_countsA, origin = \"1970-01-01\"))) %&gt;%\n    rename(date = Var1, Count = Freq) %&gt;%\n    mutate(aux_species = \"A\"),\n  as.data.frame(table(as.Date(h_countsB, origin = \"1970-01-01\"))) %&gt;%\n    rename(date = Var1, Count = Freq) %&gt;%\n    mutate(aux_species = \"B\")\n)\n\n# Convert Date column and bin by week\ndate_counts &lt;- date_counts %&gt;%\n  mutate(date = as.Date(date),\n         Week = cut(date, breaks = \"week\", start.on.monday = TRUE))\n\n# Aggregate the counts by week and species\naux_current &lt;- date_counts %&gt;%\n  group_by(Week, aux_species) %&gt;%\n  summarise(Total_Count = sum(Count), .groups = \"drop\") %&gt;%\n  mutate(Week = as.Date(Week))  # Convert Week to Date type for plotting\nPlot showing species counts of a current auxiliary catch data set.\nAgain we can calculate weekly species proportions for our auxiliary count data using the methods detailed in the last section:\naux_proportions &lt;- aux_current %&gt;%\n  group_by(Week) %&gt;%\n  mutate(total_n = sum(Total_Count),\n         proportion = Total_Count / total_n) %&gt;%\n  ungroup()\nWe can utilize those weekly proportions from the proximate auxiliary counts to assign weekly proportions of species to our sonar count data. This is based on the assumption that the proportion of a given species in the sonar count is equal to the proportion of that species in our auxiliary data:\n\\[\nP_{A,i} = \\frac{N_{A,i}}{N_i}\n\\tag{1}\\]\nWhere \\(P_{A,i}\\) is the proportion of species A during period \\(i\\), \\(N_{A,i}\\) is the number of fish of species A captured in our auxiliary data set during period \\(i\\), and \\(N_i\\) is the total number of fish captured in our auxiliary data during period \\(i\\). A side note: if the auxiliary data is some distance away from the sonar counts, we may try and utilize a lag-time in our proportion estimates, so that the catch at the hatchery during week \\(i\\) is representative sonar data from week \\(i-1\\).\nStart by joining our simulated daily_sonar data to the hatchery_proportions data we produced.\ndaily_sonar_nospecies&lt;-daily_sonar%&gt;%\n  group_by(date,Week)%&gt;%\n  summarise(Net_Movement=sum(Net_Movement))\ndaily_sonar_proportions&lt;-daily_sonar_nospecies%&gt;%\n  left_join(aux_proportions,by=\"Week\")\nIf you take a look at the daily_sonar_proportions data, you’ll see that our sonar movement data starts and ends several days outside the hatchery count date range. To deal with this we can assume that sonar movement counts before the first week of hatchery data are species A, and movement counts after the last week of hatchery data are species B. We can do that with the following code chunk:\n# Identify the first and last hatch dates\nfirst_aux_week &lt;- first(aux_proportions$Week)\nlast_aux_week &lt;- last(aux_proportions$Week)\n\ndaily_sonar_proportions$predicted_species&lt;-NA\n# Assign species based on date conditions\ndaily_sonar_proportions &lt;- daily_sonar_proportions %&gt;%\n  mutate(\n    predicted_species = case_when(\n      is.na(predicted_species) & Week &lt; first_aux_week ~ \"A\",\n      is.na(predicted_species) & Week &gt; last_aux_week ~ \"B\",\n      is.na(predicted_species) ~ aux_species \n    ),\n    proportion = if_else(is.na(proportion), 1, proportion)\n  )\ndaily_sonar_proportions&lt;-daily_sonar_proportions%&gt;%\n  mutate(assigned_count=round(proportion*Net_Movement))\nNext we can assign a proportion of each days sonar movement count to either of the two species based on the proportion of that species from the hatchery count data for that week.\nweekly_sonar_proportions&lt;-dplyr::select(daily_sonar_proportions,\n                                       Week,date,\n                                       species=predicted_species,\n                                       assigned_count)\nPlotting the above data, we get results similar to what we had with the in-season cutoff, but here we have proportional data that shows overlap in species counts, so that we aren’t assuming there is no overlap in run timing.\nPlot showing species assignments of sonar data using a proportional method\nThe above method of speciation is still relatively simple in it’s execution, and relies on the following assumptions:\nNow we can use the speciation above to estimate our seasonal abundance for each of the two species:\nN_est2&lt;-weekly_sonar_proportions%&gt;%\n  group_by(species)%&gt;%\n  summarise(total=sum(assigned_count))\nN_est2\n\n# A tibble: 2 × 2\n  species total\n  &lt;chr&gt;   &lt;dbl&gt;\n1 A        1057\n2 B         943\nReferencing our original simulated sonar data, we know we had 1000 individuals for species A and 850 for species B, so our accuracy in our end abundance estimates is pretty good. However we won’t know the true numbers when using real data, so we’ll need a way to estimate uncertainty."
  },
  {
    "objectID": "inseason_proportion.html#estimating-uncertainty",
    "href": "inseason_proportion.html#estimating-uncertainty",
    "title": "In-season species proportion",
    "section": "Estimating uncertainty",
    "text": "Estimating uncertainty\nWe can expand upon the in-season proportion method created above by using iterative “bootstrapping” to generate confidence intervals of our estimates. This is done iteratively re-sampling our auxiliary hatchery count data to assign species counts to our sonar data. This process is repeated for a set number of iterations to generate variance, which is then used to estimate confidence intervals. One advantage of bootstrapping is that it does not rely on assumptions of normality, making it a robust method for data with unknown distributions.\nWe can begin the bootstrapping by writing a function that takes our auxiliary species count data (aux_proportions) for each week of the sonar survey and randomly samples the counts to assign species to the sonar movement data. (Figure 1).\n\n\n\nFigure 1: Conceptual figure of bootstrapping weekly auxiliary count data to produce variance in species assignments.\n\n\n\nsonar_sampling &lt;- function(daily_sonar, aux_data) {\n  weeks &lt;- unique(daily_sonar$Week)\n  \n  #estimate weekly proportions of auxiliary species counts\n  aux_proportions &lt;- aux_data %&gt;%\n    group_by(Week) %&gt;%\n    mutate(total_n = sum(Total_Count),\n         proportion = Total_Count / total_n) %&gt;%\n    ungroup()\n  \n  # Predefine data frame for results\n  weekly_sonar &lt;- vector(\"list\", length(weeks))\n  \n  # Get boundary weeks for aux_proportions\n  first_aux_week &lt;- min(aux_proportions$Week)\n  last_aux_week &lt;- max(aux_proportions$Week)\n  \n  for (i in seq_along(weeks)) {\n    week &lt;- weeks[i]\n    \n    # Get auxiliary data for the current week\n    wk_sp &lt;- aux_proportions %&gt;% filter(Week == week)\n    wk_sonar &lt;- daily_sonar %&gt;% filter(Week == week)\n    \n    # Create species vector or default to \"A\" or \"B\" based \n    #on week range this is for weeks of sonar data\n    #that don't match up with weeks of aux data\n    sp_vec &lt;- if (nrow(wk_sp) &gt; 0) {\n      rep(wk_sp$aux_species, wk_sp$Total_Count)\n    } else if (week &lt; first_aux_week) {\n      \"A\"\n    } else if (week &gt; last_aux_week) {\n      \"B\"\n    } else {\n      character(0)\n    }\n    \n    # Perform sampling and count occurrences of each species\n    if (length(sp_vec) &gt; 0) {\n      samp_sonar &lt;- sample(sp_vec, sum(wk_sonar$Net_Movement),\n                           replace = TRUE)\n      samp_A &lt;- sum(samp_sonar == \"A\")\n      samp_B &lt;- sum(samp_sonar == \"B\")\n    } else {\n      samp_A &lt;- 0\n      samp_B &lt;- 0\n    }\n    \n    # Store results in a list to avoid repeated data frame binding\n    weekly_sonar[[i]] &lt;- data.frame(Week = week, A = samp_A, B = samp_B)\n  }\n  \n  # Bind the list into a single data frame after the loop\n  weekly_sonar &lt;- do.call(rbind, weekly_sonar)\n  return(weekly_sonar)\n}\n\nThe above function takes our simulated data daily_sonar and aux_data and does the following:\n\nMakes a vector listing all the weeks of survey.\nCalculates the species proportions for each week of the auxiliary data.\nRuns a for loop that assigns a species to sonar counts by sampling from corresponding weeks of the auxiliary data.\nFor weeks of sonar data outside of the range of aux data, the loop assigns species A to early counts, and species B to later counts.\n\nWe can now use the above sonar_sampling function to “bootstrap” our data to estimate the distribution of our species counts generated with some sampling error. For this example, we repeat the sampling process using our function for 25 iterations in a simple for loop. When running bootstrapping yourself, you’ll likely want to aim for 500 to 1000 iterations, which will take quite a bit longer to run.\n\n#bootstrapping boogie\niterations&lt;-100\nresults&lt;-data.frame()\n\nfor(j in 1:iterations){\n  iter&lt;-data.frame('iteration'=j)\n  d&lt;-iter%&gt;%cbind(sonar_sampling(daily_sonar,aux_current))\n  results&lt;-results%&gt;%rbind(d)\n}\n\nhead(results)\n\n  iteration       Week  A B\n1         1 2023-08-28  1 0\n2         1 2023-09-04  3 0\n3         1 2023-09-11  5 0\n4         1 2023-09-18 11 0\n5         1 2023-09-25 19 0\n6         1 2023-10-02 33 0\n\n\nThe results output of our bootstrapping shows weekly species counts for sonar data produced by our random sampling of auxiliary data. We can group the results together by iteration for an estimate of species total for each season, and then calculate lower and upper 95% confidence intervals using the quantile() call.\n\n#iteration total estimates\niter_totals&lt;-results%&gt;%\n  group_by(iteration)%&gt;%\n  dplyr::summarise(A=sum(A),\n                   B=sum(B))\n\n#bounds\nA_stats &lt;- quantile(iter_totals$A, probs = c(0.025, 0.975))\nB_stats &lt;- quantile(iter_totals$B, probs = c(0.025, 0.975))\nA_mean&lt;-mean(iter_totals$A)\nB_mean&lt;-mean(iter_totals$B)\n\nUsing the above we see that our estimate of total abundance of species A in our sonar counts is 1057 with 95% CI [1043, 1078] and a count of 943 with 95% CI [922, 958] for species B.\nWe can also estimate 95% confidence intervals for our weekly species assignments using the results output.\n\n# Weekly total estimates with confidence intervals\nweekly_results &lt;- results %&gt;%\n  group_by(Week) %&gt;%\n  summarise(\n    A_mean = mean(A),\n    A_lb = quantile(A, probs = 0.025),\n    A_ub = quantile(A, probs = 0.975),\n    B_mean = mean(B),\n    B_lb = quantile(B, probs = 0.025),\n    B_ub = quantile(B, probs = 0.975),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_longer(\n  cols = 2:7,\n  names_to = c(\"Species\", \".value\"),\n  names_sep = \"_\")\n\n\n\n\n\n\nPlot showing species assignments of sonar data using proportional bootstrapping to estimate confidence intervals (95%).\n\n\n\n\nThe above figure shows that during periods of run-timing overlap, our precision in estimation of species assignment decreases substantially. The more overlap in run-timing, the more bias we can expect in our speciation.\nThe above method of speciation provides a straightforward approach for leveraging more contemporary auxiliary data to infer species composition in sonar detection studies. Using the species proportions from this auxiliary data set we assign species proportions to sonar data, and can iteratively resample the data through “bootstraping” to estimate confidence intervals. Key assumptions to realize when utilizing this method are:\n\nAll fish being speciated are either one of two species.\nSpecies proportions in the auxiliary data is representative of the fish community at the sonar site.\n\nWhen considering auxiliary data sets to use for informing species apportionment, the more proximate the data to the sonar site, the more accurate species assignments will likely be."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Objectives",
    "section": "",
    "text": "This is a draft write up that aims to provide examples of ways auxiliary data can be used to inform the speciation of sonar fish imaging data. This is part of an ongoing effort by the California (Coastal) Monitoring Plan (CMP) Sonar Subgroup to provide support to survey efforts using sonar to estimate abundances of migrating adult salmonids. Often distinguishing species for a given fish using a sonar image can be difficult due to images that are low-resolution and at an angle that makes identification difficult. The difficulty of accurate speciation can be compounded in systems when many different species are present at the same time, especially if those fish are of similar size classes and produce similar acoustic profiles.\nI do not aim to provide a “blanket” method for species apportionment that can be applied to all sonar imaging studies. Each watershed is different, and has different auxiliary data available for use. Also, the methods detailed do not aim to replace visual identifications made by scientists with local knowledge and identification experience. Instead, this document will provide examples of statistical methods that can utilize auxiliary data to aid in assigning species identifications to sonar movement data where visual identification is not possible. Some of these methods have been briefly described in Atkinson, Lacy, and Bellmer (2016), and I hope the following methodology provides additional helpful detail on how to implement the techniques.\nWe will begin with the most simple methods, incorporating the barest of auxiliary data sets, and build upon those with increasingly more complex methods and data sets. Hopefully by the end, the reader can better evaluate how to tackle their sonar imaging data and what auxiliary data they can use to increase the accuracy of species apportionment.\nThis will be a “living” document, with continues updates and edits being made while we continue to explore and test various speciation methods. Throughout this document, various chunks of R code will be presented in the following format:\n\nprint(\"hello reader\")\n\n[1] \"hello reader\"\n\n\nThese chunks are designed for users to copy and paste, or rewrite entirely, into their own R scripts to replicate the methods.\n\n\n\n\n\nReferences\n\nAtkinson, Kristine, Michael K Lacy, and Russell Bellmer. 2016. “Dual frequency identification sonar (DIDSON) deployment and preliminary performance as part of the California coastal salmonid monitoring plan.”"
  }
]